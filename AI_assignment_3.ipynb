{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "Assignment 3\n",
        "\n",
        "COMP 5600/6600\n",
        "\n",
        "Due: 10/25/2023 11:59 pm\n",
        "\n",
        "[Question 1]\n",
        "\n",
        "[15 points]\n",
        "\n",
        "Design a convolutional neural network in Keras of at least 10 convolutional layers. Use the MNIST\n",
        "dataset for evaluation. You must try three designs as detailed below and provide your\n",
        "observations on the performance of each:\n",
        "1. A regular CNN where the number of filters in each layer increases as the depth of the\n",
        "network grows i.e., the Lth layer will have more filters than the (L-1)th layer.\n",
        "2. An inverted CNN where the number of filters in each layer decreases as the depth of the\n",
        "network grows i.e., the Lth layer will have less filters than the (L-1)th layer.\n",
        "3. An hour-glass shaped CNN where the number of filters will increase till the Lth layer and\n",
        "reduce afterwards.\n",
        "Your goal is to design these networks and optimize them to their best performance by choosing\n",
        "the right hyperparameters for each network, such as the learning rate, batch size and the choice\n",
        "of optimizer (‘SGD’, ‘adam’, ‘RMSProp’). You must provide a detailed report of what values you\n",
        "tried for each hyperparameters, your observations on why the network performed well (or not)\n",
        "and the final accuracy for each network on the MNIST dataset.\n",
        "You can refer to the Keras documentation for more details"
      ],
      "metadata": {
        "id": "2st0m4Thd3Y4"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report:\n",
        "\n",
        "Whening testing different hyperparameters I started by testing different learing rate. I tried four different learning rates 0.1, 0.01, 0.001, 0.0001\n",
        "the first three values seemed to not give the model enough time to learn from the training data resulting in very low accuracy scores and very high loss scores I ended up using 0.0001 because it gave the model enough time to learn from the training data to result in a high accuracy and low lose scores when used on the test data. Next I tried different batch sizes smaller batch sizes such as 32 made the trianing run slower and resulted in a larger disparity between the trianing data accuracy. On the other side high batch sizes such as 128 and 512 resulted in the training of the model running faster but overfit the data resulting in a very low accuracy when tested on the test data. When testing different opimizers they all had similar performaces but 'adam' would always test with the highest accuracy.Because of the previously mentioned testing I settled on using a learing rate of 0.0001 a batch size of 64 and 'Adam' as my choice of optimizer this resulted in my test accuracy ,for my first type of CNN architecture where layer l has more filters then layer l-1, being about 0.9859 then for my CNN with the architecture of layer l having fewer layer then layer l-1 having a test accuracy of 0.9768 and then finaly for my CNN that should be hour glass shaped resulted in a test accuracy of 0.9839."
      ],
      "metadata": {
        "id": "ZPrp2HUD7lLX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train, x_test = np.expand_dims(x_train, axis=-1), np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "num_filters = 8\n",
        "\n",
        "#adding first layer to the model\n",
        "model.add(layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', input_shape=(28,28,1)))\n",
        "\n",
        "#adding next nine layers to the model and doubling the number of filters for each layer\n",
        "for i in range (9):\n",
        "  num_filters *= 2\n",
        "  model.add(layers.Conv2D(num_filters, (3,3), activation='relu', padding='same'))\n",
        "\n",
        "#flattening model to make a one diminsional array\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#training model\n",
        "model.fit(x_train, y_train, epochs=1, batch_size=64, validation_data=(x_test,y_test))\n",
        "\n",
        "#evaluating model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss: {0} - Test Acc: {1}\".format(test_loss, test_acc))"
      ],
      "metadata": {
        "id": "B8sY18mAeAj5",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9f853f10-7b3d-4d68-a3cd-b6b14ed202fc"
      },
      "execution_count": null,
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/mnist.npz\n",
            "11490434/11490434 [==============================] - 0s 0us/step\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train, x_test = np.expand_dims(x_train, axis=-1), np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "num_filters = 4096\n",
        "\n",
        "#adding first layer to the model\n",
        "model.add(layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', input_shape=(28,28,1)))\n",
        "\n",
        "#adding next nine layers to the model and doubling the number of filters for each layer\n",
        "for i in range (9):\n",
        "  num_filters = num_filters/2\n",
        "  model.add(layers.Conv2D(num_filters, (3,3), activation='relu', padding='same'))\n",
        "\n",
        "#flattening model to make a one diminsional array\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#training model\n",
        "model.fit(x_train, y_train, epochs=1, batch_size=64, validation_data=(x_test,y_test))\n",
        "\n",
        "#evaluating model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss: {0} - Test Acc: {1}\".format(test_loss, test_acc))"
      ],
      "metadata": {
        "id": "nZh3Ud3IxThz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1aa560b4-f728-4d7a-d750-ea4ef169c145"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 4744s 5s/step - loss: 0.1739 - accuracy: 0.9466 - val_loss: 0.0751 - val_accuracy: 0.9768\n",
            "313/313 [==============================] - 126s 404ms/step - loss: 0.0751 - accuracy: 0.9768\n",
            "Test Loss: 0.07507205754518509 - Test Acc: 0.9768000245094299\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.datasets import mnist\n",
        "\n",
        "# Load and preprocess the MNIST dataset\n",
        "(x_train, y_train), (x_test, y_test) = mnist.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "x_train, x_test = np.expand_dims(x_train, axis=-1), np.expand_dims(x_test, axis=-1)\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "num_filters = 32\n",
        "\n",
        "#adding first layer to the model\n",
        "model.add(layers.Conv2D(num_filters, (3,3), activation='relu', padding='same', input_shape=(28,28,1)))\n",
        "\n",
        "#adding next nine layers to the model and doubling the number of filters for each layer\n",
        "for i in range (4):\n",
        "  num_filters *= 2\n",
        "  model.add(layers.Conv2D(num_filters, (3,3), activation='relu', padding='same'))\n",
        "\n",
        "for i in range (5):\n",
        "  num_filters /= 2\n",
        "  model.add(layers.Conv2D(num_filters, (3,3), activation='relu', padding='same'))\n",
        "\n",
        "#flattening model to make a one diminsional array\n",
        "model.add(layers.Flatten())\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.0001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#training model\n",
        "model.fit(x_train, y_train, epochs=1, batch_size=64, validation_data=(x_test,y_test))\n",
        "\n",
        "#evaluating model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss: {0} - Test Acc: {1}\".format(test_loss, test_acc))"
      ],
      "metadata": {
        "id": "zy8FAK9NxTwu",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "df4176dd-7346-4dc1-fe1a-c061386fea5d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "938/938 [==============================] - 106s 109ms/step - loss: 0.1768 - accuracy: 0.9463 - val_loss: 0.0534 - val_accuracy: 0.9839\n",
            "313/313 [==============================] - 6s 18ms/step - loss: 0.0534 - accuracy: 0.9839\n",
            "Test Loss: 0.053366754204034805 - Test Acc: 0.9839000105857849\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 2]\n",
        "\n",
        " [25 points]\n",
        "\n",
        "Implement the LeNet Convolutional Neural Network using Keras. It is a seven-layer network with\n",
        "three convolutional layers, two max-pooling layers and 2 dense layers. The structure is shown\n",
        "below:\n",
        "\n",
        "Layer 1: convolution layer with 6 convolution kernels of 5x5 with stride 1\n",
        "\n",
        "Layer 2: max-pooling layer with 2x2 kernels with stride 2\n",
        "\n",
        "Layer 3: convolution layer with 16 convolution kernels of 5x5 with stride 1\n",
        "\n",
        "Layer 4: max-pooling layer with 2x2 kernels with stride 2\n",
        "\n",
        "Layer 5: convolution layer with 120 convolution kernels of 5x5\n",
        "\n",
        "Layer 6: dense layer with 84 neurons\n",
        "\n",
        "Layer 7: output layer\n",
        "\n",
        "Use the ‘Adam’ optimizer to train your network on the CIFAR-10 dataset for a fixed set of 25\n",
        "epochs. You can use the built-in functions to load the data. Each image is 32x32x3 matrix and you\n",
        "will have 60,000 images for training and 10,000 for test. There are 10 classes in the dataset each\n",
        "representing an object in the image.\n",
        "Perform the following analysis and answer each question briefly (3-5 sentences). Use plots and\n",
        "figures as necessary.\n",
        "1. What is the effect of learning rate on the training process? Which performed best?\n",
        "2. What is the effect of batch size on the training process? Which performed best?\n",
        "3. Try different hyperparameters to obtain the best accuracy on the test set. What is your\n",
        "best performance and what were the hyperparameters?\n",
        "4. Implement an equivalent feed forward network for the same task with each hidden layer\n",
        "containing the same number of neurons as the number of filters in each convolution layer.\n",
        "Use the ‘Adam’ optimizer to train your network on the CIFAR-10 dataset for a fixed set of\n",
        "25 epochs. Compare its performance with your LeNet implementation based on the\n",
        "following questions:\n",
        "a. What is its performance?\n",
        "b. How many parameters are there in this network compared to the LeNet\n",
        "implementation? Are they worth it?"
      ],
      "metadata": {
        "id": "sXAMrUW7eBC2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Report:\n",
        "1. the effect of the learning rate is how fast the model learns from the data during training. During my expirementation I tried learing rates of 0.1 and 0.01 which seemed to underfit the data leading to the model making very inaccurate predictions. I also tried 0.0001 which resulted in the model overfitting the data during trianing and also leading to a low accuracy in testing. I ended up using 0.001 which resulted in the best accuracy and seemed to fit the data well.\n",
        "2. When testing different batch sizes I noticed that the small batch sizes(such as 32 and 64) slowed down the training process but were able to capture features from the data better shown in higher accuracy scores where as having to large of a batch size(such as 128, 512, and 1024) increased the speed at which the training completed but lead to the model working well for the training data but preforming poorly on the actual test data.Because of this I settled on the batch size of 32.\n",
        "3. I tried different different hyperparameters but ended up using a learning rate of 0.001 and a batch size of 32 with the specified acritecture above and the best accuracy from training I recieved was 0.7594 with a loss of 0.6752 and during testing I recieved an accuracy of 0.6030 with a loss of 1.3338.\n",
        "4.\n",
        "  1) When implementing the feed forward network the performance was signifagantly worse than the lenet network, with the accuracy for the lenet being 0.6030 compared to the accuracy of the feed forward network being 0.2088\n",
        "  \n",
        "  2) because of the conflusion of the previous question and the fact that the lenet nework as more trainable parameters than the feed forward netowrk i would say that the extra parameters are worth it since they resulted in the higher accuracy and lower loss of data"
      ],
      "metadata": {
        "id": "R3BLsupp7i1w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# Load and preprocess the cifar10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "#adding first layer to the model\n",
        "model.add(layers.Conv2D(6, (5, 5), activation='relu', padding='valid', strides=(1, 1), input_shape=(32, 32, 3)))\n",
        "\n",
        "#second layer\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "#third layer\n",
        "model.add(layers.Conv2D(16, (5, 5), activation='relu', padding='valid', strides=(1, 1)))\n",
        "\n",
        "#fourth layer\n",
        "model.add(layers.MaxPooling2D((2, 2), strides=(2, 2)))\n",
        "\n",
        "#fifth layer\n",
        "model.add(layers.Conv2D(120, (5, 5), activation='relu', padding='valid'))\n",
        "\n",
        "#flattening model to make a one diminsional array\n",
        "model.add(layers.Flatten())\n",
        "\n",
        "#sixth layer\n",
        "model.add(layers.Dense(84, activation='relu'))\n",
        "\n",
        "#seventh layer\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#training model\n",
        "model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_test,y_test))\n",
        "\n",
        "#evaluating model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss: {0} - Test Acc: {1}\".format(test_loss, test_acc))"
      ],
      "metadata": {
        "id": "RJSodi_geGnx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4be21c48-963d-4f63-e89d-e16e191fe310"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Downloading data from https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz\n",
            "170498071/170498071 [==============================] - 4s 0us/step\n",
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 12s 5ms/step - loss: 1.6083 - accuracy: 0.4152 - val_loss: 1.3949 - val_accuracy: 0.4888\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.3356 - accuracy: 0.5214 - val_loss: 1.3898 - val_accuracy: 0.5052\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 1.2217 - accuracy: 0.5630 - val_loss: 1.1993 - val_accuracy: 0.5734\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.1472 - accuracy: 0.5915 - val_loss: 1.1876 - val_accuracy: 0.5809\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 1.0856 - accuracy: 0.6156 - val_loss: 1.2010 - val_accuracy: 0.5774\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 1.0391 - accuracy: 0.6325 - val_loss: 1.1329 - val_accuracy: 0.6009\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9944 - accuracy: 0.6486 - val_loss: 1.1039 - val_accuracy: 0.6120\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.9548 - accuracy: 0.6615 - val_loss: 1.1333 - val_accuracy: 0.6072\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.9203 - accuracy: 0.6733 - val_loss: 1.1159 - val_accuracy: 0.6168\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.8843 - accuracy: 0.6861 - val_loss: 1.1297 - val_accuracy: 0.6120\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.8564 - accuracy: 0.6966 - val_loss: 1.1239 - val_accuracy: 0.6201\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8271 - accuracy: 0.7079 - val_loss: 1.1510 - val_accuracy: 0.6170\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.8033 - accuracy: 0.7130 - val_loss: 1.1496 - val_accuracy: 0.6123\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7824 - accuracy: 0.7204 - val_loss: 1.1908 - val_accuracy: 0.6107\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7556 - accuracy: 0.7332 - val_loss: 1.2203 - val_accuracy: 0.6029\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.7320 - accuracy: 0.7377 - val_loss: 1.2231 - val_accuracy: 0.6095\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.7136 - accuracy: 0.7468 - val_loss: 1.2310 - val_accuracy: 0.6106\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6955 - accuracy: 0.7507 - val_loss: 1.3048 - val_accuracy: 0.6038\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 0.6808 - accuracy: 0.7554 - val_loss: 1.3014 - val_accuracy: 0.6061\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6609 - accuracy: 0.7631 - val_loss: 1.3308 - val_accuracy: 0.5968\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6453 - accuracy: 0.7683 - val_loss: 1.3324 - val_accuracy: 0.6026\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6281 - accuracy: 0.7755 - val_loss: 1.3325 - val_accuracy: 0.6063\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.6114 - accuracy: 0.7820 - val_loss: 1.3485 - val_accuracy: 0.5991\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 0.6007 - accuracy: 0.7843 - val_loss: 1.3808 - val_accuracy: 0.6005\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 0.5845 - accuracy: 0.7885 - val_loss: 1.4148 - val_accuracy: 0.6030\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 1.4148 - accuracy: 0.6030\n",
            "Test Loss: 1.4148222208023071 - Test Acc: 0.6029999852180481\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "from keras import layers, models\n",
        "from keras.datasets import cifar10\n",
        "\n",
        "# Load and preprocess the cifar10 dataset\n",
        "(x_train, y_train), (x_test, y_test) = cifar10.load_data()\n",
        "x_train, x_test = x_train / 255.0, x_test / 255.0\n",
        "\n",
        "model = models.Sequential()\n",
        "\n",
        "#feed forward NN implementation\n",
        "#flattening model to make a one diminsional array\n",
        "model.add(layers.Flatten(input_shape=(32,32,3)))\n",
        "\n",
        "model.add(layers.Dense(6, activation='relu'))\n",
        "model.add(layers.Dense(16, activation='relu'))\n",
        "model.add(layers.Dense(120, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n",
        "model.compile(optimizer=optimizer, loss='sparse_categorical_crossentropy', metrics=['accuracy'])\n",
        "\n",
        "#training model\n",
        "model.fit(x_train, y_train, epochs=25, batch_size=32, validation_data=(x_test,y_test))\n",
        "\n",
        "#evaluating model\n",
        "test_loss, test_acc = model.evaluate(x_test, y_test)\n",
        "print(\"Test Loss: {0} - Test Acc: {1}\".format(test_loss, test_acc))"
      ],
      "metadata": {
        "id": "BV_GtK1EZKHP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "dbbbb546-0c22-4223-e38a-a9bbbd64a27a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "1563/1563 [==============================] - 8s 4ms/step - loss: 2.0874 - accuracy: 0.1819 - val_loss: 2.0410 - val_accuracy: 0.2036\n",
            "Epoch 2/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0468 - accuracy: 0.1922 - val_loss: 2.0261 - val_accuracy: 0.2002\n",
            "Epoch 3/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0385 - accuracy: 0.1978 - val_loss: 2.0272 - val_accuracy: 0.2060\n",
            "Epoch 4/25\n",
            "1563/1563 [==============================] - 8s 5ms/step - loss: 2.0329 - accuracy: 0.2007 - val_loss: 2.0235 - val_accuracy: 0.2026\n",
            "Epoch 5/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0337 - accuracy: 0.2028 - val_loss: 2.0433 - val_accuracy: 0.1943\n",
            "Epoch 6/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0320 - accuracy: 0.2013 - val_loss: 2.0378 - val_accuracy: 0.2033\n",
            "Epoch 7/25\n",
            "1563/1563 [==============================] - 7s 5ms/step - loss: 2.0302 - accuracy: 0.2051 - val_loss: 2.0163 - val_accuracy: 0.2098\n",
            "Epoch 8/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0279 - accuracy: 0.2055 - val_loss: 2.0223 - val_accuracy: 0.2020\n",
            "Epoch 9/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0272 - accuracy: 0.2049 - val_loss: 2.0192 - val_accuracy: 0.2120\n",
            "Epoch 10/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0283 - accuracy: 0.2049 - val_loss: 2.0164 - val_accuracy: 0.2099\n",
            "Epoch 11/25\n",
            "1563/1563 [==============================] - 7s 4ms/step - loss: 2.0280 - accuracy: 0.2046 - val_loss: 2.0348 - val_accuracy: 0.2014\n",
            "Epoch 12/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0273 - accuracy: 0.2060 - val_loss: 2.0198 - val_accuracy: 0.2073\n",
            "Epoch 13/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0248 - accuracy: 0.2052 - val_loss: 2.0144 - val_accuracy: 0.2092\n",
            "Epoch 14/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0276 - accuracy: 0.2059 - val_loss: 2.0177 - val_accuracy: 0.2131\n",
            "Epoch 15/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0262 - accuracy: 0.2041 - val_loss: 2.0442 - val_accuracy: 0.1906\n",
            "Epoch 16/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0256 - accuracy: 0.2055 - val_loss: 2.0145 - val_accuracy: 0.2065\n",
            "Epoch 17/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0236 - accuracy: 0.2074 - val_loss: 2.0163 - val_accuracy: 0.2109\n",
            "Epoch 18/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0241 - accuracy: 0.2060 - val_loss: 2.0291 - val_accuracy: 0.2050\n",
            "Epoch 19/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0233 - accuracy: 0.2061 - val_loss: 2.0180 - val_accuracy: 0.2086\n",
            "Epoch 20/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0237 - accuracy: 0.2078 - val_loss: 2.0170 - val_accuracy: 0.2109\n",
            "Epoch 21/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0241 - accuracy: 0.2049 - val_loss: 2.0202 - val_accuracy: 0.2064\n",
            "Epoch 22/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0227 - accuracy: 0.2075 - val_loss: 2.0343 - val_accuracy: 0.1980\n",
            "Epoch 23/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0230 - accuracy: 0.2067 - val_loss: 2.0159 - val_accuracy: 0.2085\n",
            "Epoch 24/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0243 - accuracy: 0.2057 - val_loss: 2.0183 - val_accuracy: 0.2052\n",
            "Epoch 25/25\n",
            "1563/1563 [==============================] - 6s 4ms/step - loss: 2.0214 - accuracy: 0.2091 - val_loss: 2.0134 - val_accuracy: 0.2088\n",
            "313/313 [==============================] - 1s 2ms/step - loss: 2.0134 - accuracy: 0.2088\n",
            "Test Loss: 2.0134339332580566 - Test Acc: 0.20880000293254852\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "[Question 3]\n",
        "\n",
        " [10 points]\n",
        "\n",
        "Consider the below matrices as input (X) and convolutional kernel f. Consider that the depth of\n",
        "the input is 1.\n",
        "\n",
        "\n",
        "\n",
        "Compute the following:\n",
        "1. What are the dimensions of the input and the kernel (or filter)? How many parameters\n",
        "are there in the kernel f? [2 points]\n",
        "2. What is the output activation map when you apply the convolutional operation using the\n",
        "filter f on the input X without padding? [4 points]\n",
        "3. What is the output when you apply a max-pooling operation on the output from the\n",
        "previous question? [4 points]\n",
        "\n",
        "Note: For parts 2 and 3 in question 3, please provide the actual output. You can work this one\n",
        "out by hand or write your own code to do it. Provide documentation of how you got the outputs.\n"
      ],
      "metadata": {
        "id": "rff3W_qceHM_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "\n",
        "input_matrix = np.array([[7, 5, 0, 0, 3, 2],\n",
        "                         [6, 4, 5, 1, 4, 8],\n",
        "                         [9, 0, 2, 2, 5, 4],\n",
        "                         [6, 3, 4, 7, 9, 8],\n",
        "                         [5, 7, 5, 6, 9, 0],\n",
        "                         [7, 9, 0, 8, 2, 3]])\n",
        "\n",
        "filter_matrix = np.array([[1, 0, -1],\n",
        "                          [2, 0, -2],\n",
        "                          [1, 0, -1]])\n",
        "\n",
        "# Code for Question 1\n",
        "input_height, input_width = input_matrix.shape\n",
        "filter_height, filter_width = filter_matrix.shape\n",
        "num_parameters = filter_height * filter_width\n",
        "print(\"Question 1:\")\n",
        "print(f\"Input Dimensions: ({input_height}, {input_width})\")\n",
        "print(f\"Filter Dimensions: ({filter_height}, {filter_width})\")\n",
        "print(f\"Number of Parameters in the Filter: {num_parameters}\")\n",
        "\n",
        "# Code for Question 2\n",
        "output_matrix = np.zeros((input_height - filter_height + 1, input_width - filter_width + 1))\n",
        "for i in range(input_height - filter_height + 1):\n",
        "    for j in range(input_width - filter_width + 1):\n",
        "        output_matrix[i, j] = np.sum(input_matrix[i:i+filter_height, j:j+filter_width] * filter_matrix)\n",
        "\n",
        "print(\"Question 2:\")\n",
        "print(\"Output Matrix after Convolution:\")\n",
        "print(output_matrix)\n",
        "\n",
        "# Code for Question 3\n",
        "pooling_size = 2  # Pooling window size\n",
        "output_height, output_width = output_matrix.shape\n",
        "pooled_matrix = np.zeros((output_height // pooling_size, output_width // pooling_size))\n",
        "\n",
        "for i in range(0, output_height, pooling_size):\n",
        "    for j in range(0, output_width, pooling_size):\n",
        "        pooled_matrix[i // pooling_size, j // pooling_size] = np.max(output_matrix[i:i+pooling_size, j:j+pooling_size])\n",
        "\n",
        "print(\"Question 3:\")\n",
        "print(\"Output Matrix after Max-Pooling:\")\n",
        "print(pooled_matrix)\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GWmhuvy4J_6l",
        "outputId": "ef359c7a-5e8d-4b12-b608-a15b4dc0f810"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Question 1:\n",
            "Input Dimensions: (6, 6)\n",
            "Filter Dimensions: (3, 3)\n",
            "Number of Parameters in the Filter: 9\n",
            "Question 2:\n",
            "Output Matrix after Convolution:\n",
            "[[ 16.   9.  -4. -18.]\n",
            " [ 17.  -5. -10. -12.]\n",
            " [ 11.  -9. -17.   2.]\n",
            " [  9.  -1. -15.  16.]]\n",
            "Question 3:\n",
            "Output Matrix after Max-Pooling:\n",
            "[[17. -4.]\n",
            " [11. 16.]]\n"
          ]
        }
      ]
    }
  ]
}